{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLandforms Training YOLOv5\n",
    "## Landforms object detection training\n",
    "@ g.nodjoumi@jacobs-university.de\n",
    "\n",
    "***This notebook is based on yolov5 repository is provided by ultralytics https://zenodo.org/record/4154370 that has been modified to adapt to landforms***\n",
    "\n",
    "## Dataset\n",
    "If you are using Roboflow prepared dataset skip this part, else prepare the dataset folder as follow:\n",
    "\n",
    "Organize the dataset folder as following:\n",
    "\n",
    "* *datasetfolder/train/images*\n",
    "* *datasetfolder/valid/images*\n",
    "\n",
    "then create a dataset.yaml file containing:\n",
    "\n",
    "* train: *path to datasetfolder/train/images*\n",
    "* valid: *path to datasetfolder/valid/images*\n",
    "* nc: *number of classes*\n",
    "* names: ['label1','label2','...']\n",
    "\n",
    "## How-TO\n",
    "Just run the notebook and it will ask all variables and parameters necessary for training.\n",
    "\n",
    "*To further personalize the training, edit the dictionary, in the dedicated cell below, before run the notebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd yolov5\n",
    "#!pip install -U -r requirements.txt  # install dependencies\n",
    "import os\n",
    "from adds.GenUtils import question, askPath, askFile, askInt\n",
    "from adds.DLUtils import get_train_cfg, link_dataset, get_model_cfg, get_args\n",
    "import torch\n",
    "from IPython.display import clear_output, Image\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.7.1 _CudaDeviceProperties(name='Quadro RTX 4000', major=7, minor=5, total_memory=7973MB, multi_processor_count=36)\n"
     ]
    }
   ],
   "source": [
    "print('torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Insert path to Dataset folder /media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/Dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/Dataset is valid\n"
     ]
    }
   ],
   "source": [
    "home = os.getcwd()\n",
    "logdir = os.path.dirname(home)+'/train_logs/exp'\n",
    "models_path = home+'/models'\n",
    "source = askPath('Dataset folder')\n",
    "#os.chdir(source)\n",
    "yaml = source+'/data.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: train/images\n",
      "val: valid/images\n",
      "\n",
      "nc: 5\n",
      "names: ['1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "%cat $yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter only: \n",
      "1, 2, 3, 4\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please select 1: yolov5s, 2:yolov5m, 3: yolov5l, 4:yolov5x  Answer:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter only: \n",
      "yes, y, no, n\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "trained_weights Folder exist, remove it?   Answer:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_weights Folder created\n"
     ]
    }
   ],
   "source": [
    "model = int(question('Please select 1: yolov5s, 2:yolov5m, 3: yolov5l, 4:yolov5x', ['1','2','3','4']))\n",
    "train_cfg, weights, name = get_train_cfg(home, yaml, models_path, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create symlink between repository and data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: '/media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/Dataset/train' -> '/media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/yolov5/train'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    link_dataset(home,source)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Insert integer for Epochs:  1\n",
      "Insert integer for Image size:  64\n",
      "Insert integer for Batch size:  2\n"
     ]
    }
   ],
   "source": [
    "epochs = askInt('Epochs: ')\n",
    "img_size = askInt('Image size: ')\n",
    "batch_size = askInt('Batch size: ')\n",
    "\n",
    "if img_size <= 32:\n",
    "    img_size = 32+1\n",
    "if batch_size <4:\n",
    "    batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary with all parameters\n",
    "***Edit items if needed***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-043a58c07bc1>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-043a58c07bc1>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    '--log-imgs','', # int\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "args_dict = {\n",
    "    '--epochs':epochs, #integer\n",
    "    '--batch-size':batch_size, #integer\n",
    "    '--weights': '',#source+'/trained_weights', #path\n",
    "    '--cfg':train_cfg, #path\n",
    " \t'--data':yaml, #path\n",
    " \t'--hyp':'', #path\n",
    " \t'--img-size':img_size, #integer\n",
    " \t'--rect':'', #True of False, '' = False\n",
    " \t'--resume':'', #True of False, '' = False\n",
    " \t'--nosave':'', #True of False, '' = False\n",
    " \t'--notest':'', #True of False, '' = False\n",
    " \t'--noautoanchor':'', #True of False, '' = False\n",
    " \t'--evolve':'', #True of False, '' = False\n",
    " \t'--bucket':'', #True of False, '' = False\n",
    " \t'--cache-images':True, #True of False, '' = False\n",
    " \t'--image-weights':'', #True of False, '' = False\n",
    " \t'--device':'', #cuda or cpu, auto filled\n",
    " \t'--multi-scale':'', #True of False, '' = False\n",
    " \t'--single-cls':'', #True of False, '' = False\n",
    " \t'--adam':'',  #True of False, '' = False\n",
    " \t'--sync-bn':'', #True of False, '' = False\n",
    "    '--local_rank':'', #int\n",
    "    '--log-imgs':'', # int\n",
    "    '--log-artifacts':'', #True of False, '' = False\n",
    " \t'--workers':'', #integer\n",
    "    '--project':logdir, #path\n",
    "    '--name': '', #string\n",
    "    '--exist-ok':'', #True of False, '' = False\n",
    "    '--quad':''} #True of False, '' = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get arguments for training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--epochs 1 --batch-size 4 --cfg /media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/yolov5/models/custom_yolov5m.yaml --data /media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/Dataset/data.yaml --img-size 64 --cache-images --name /media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/train_logs/exp'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arv = get_args(args_dict)\n",
    "arv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/train_logs/exp'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ba4d99a3c9640581\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ba4d99a3c9640581\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 1.7.1 CUDA:0 (Quadro RTX 4000, 7973.0MB)\n",
      "\n",
      "\n",
      "Namespace(adam=False, batch_size=4, bucket='', cache_images=True, cfg='/media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/yolov5/models/custom_yolov5m.yaml', data='/media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/Dataset/data.yaml', device='', epochs=1, evolve=False, exist_ok=False, global_rank=-1, hyp='data/hyp.scratch.yaml', image_weights=False, img_size=[64, 64], local_rank=-1, log_artifacts=False, log_imgs=16, multi_scale=False, name='/media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/train_logs/exp', noautoanchor=False, nosave=False, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='/media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/train_logs/exp5', single_cls=False, sync_bn=False, total_batch_size=4, weights='yolov5s.pt', workers=8, world_size=1)\n",
      "Start Tensorboard with \"tensorboard --logdir runs/train\", view at http://localhost:6006/\n",
      "Hyperparameters {'lr0': 0.01, 'lrf': 0.2, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'warmup_bias_lr': 0.1, 'box': 0.05, 'cls': 0.5, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'mosaic': 1.0, 'mixup': 0.0}\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      5280  models.common.Focus                     [3, 48, 3]                    \n",
      "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
      "  2                -1  1     65280  models.common.C3                        [96, 96, 2]                   \n",
      "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
      "  4                -1  1    629760  models.common.C3                        [192, 192, 6]                 \n",
      "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
      "  6                -1  1   2512896  models.common.C3                        [384, 384, 6]                 \n",
      "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
      "  8                -1  1   1476864  models.common.SPP                       [768, 768, [5, 9, 13]]        \n",
      "  9                -1  1   4134912  models.common.C3                        [768, 768, 2, False]          \n",
      " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1   1182720  models.common.C3                        [768, 384, 2, False]          \n",
      " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1    296448  models.common.C3                        [384, 192, 2, False]          \n",
      " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1   1035264  models.common.C3                        [384, 384, 2, False]          \n",
      " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   4134912  models.common.C3                        [768, 768, 2, False]          \n",
      " 24      [17, 20, 23]  1     40410  models.yolo.Detect                      [5, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
      "Model Summary: 391 layers, 21072570 parameters, 21072570 gradients, 50.5 GFLOPS\n",
      "\n",
      "Transferred 59/506 items from yolov5s.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "Optimizer groups: 86 .bias, 86 conv.weight, 83 other\n",
      "Scanning 'train/labels.cache' for images and labels... 405 found, 0 missing, 0 e\n",
      "Caching images (0.0GB): 100%|████████████████| 405/405 [00:00<00:00, 877.92it/s]\n",
      "Scanning 'valid/labels.cache' for images and labels... 38 found, 0 missing, 0 em\n",
      "Caching images (0.0GB): 100%|██████████████████| 38/38 [00:00<00:00, 567.91it/s]\n",
      "Plotting labels... \n",
      "\n",
      "Analyzing anchors... anchors/target = 1.75, Best Possible Recall (BPR) = 0.8159. Attempting to improve anchors, please wait...\n",
      "WARNING: Extremely small objects found. 204 of 1173 labels are < 3 pixels in width or height.\n",
      "Running kmeans for 9 anchors on 1146 points...\n",
      "thr=0.25: 0.9821 best possible recall, 6.53 anchors past thr\n",
      "n=9, img_size=64, metric_all=0.443/0.853-mean/best, past_thr=0.554-mean: 2,2,  3,3,  5,5,  6,6,  9,8,  12,12,  17,15,  26,24,  39,33\n",
      "Evolving anchors with Genetic Algorithm: fitness = 0.8719: 100%|█| 1000/1000 [00\n",
      "thr=0.25: 0.9821 best possible recall, 6.56 anchors past thr\n",
      "n=9, img_size=64, metric_all=0.446/0.855-mean/best, past_thr=0.556-mean: 2,2,  3,4,  5,4,  6,6,  8,8,  12,11,  15,16,  26,24,  38,33\n",
      "New anchors saved to model. Update model *.yaml to use these anchors in the future.\n",
      "\n",
      "Image sizes 64 train, 64 test\n",
      "Using 4 dataloader workers\n",
      "Logging results to /media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/train_logs/exp5\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
      "       0/0     2.97G    0.1718  0.001881     0.049    0.2227         1        64\n",
      "               Class      Images     Targets           P           R      mAP@.5\n",
      "                 all          38         128           0           0    0.000512    5.59e-05\n",
      "Optimizer stripped from /media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/train_logs/exp5/weights/last.pt, 42.4MB\n",
      "Optimizer stripped from /media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/train_logs/exp5/weights/best.pt, 42.4MB\n",
      "1 epochs completed in 0.004 hours.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir(home)\n",
    "clear_output\n",
    "!python train.py $arv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/gnodj/W-DATS/python_scripts/DeepLearning/DeepLandforms-YOLOv5/yolov5/trained_weights/custom_yolov5m.pt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.plots import plot_results \n",
    "#plot_results(save_dir='runs/train/exp')  # plot all results*.txt as results.png\n",
    "#Image(filename='runs/train/exp/results.png', width=800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
